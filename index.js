#!/usr/bin/env node
import { Server } from "@modelcontextprotocol/sdk/server/index.js";
import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";
import { CallToolRequestSchema, ListToolsRequestSchema } from "@modelcontextprotocol/sdk/types.js";
import Groq from "groq-sdk";
import dotenv from 'dotenv';

dotenv.config();

const groq = new Groq({ apiKey: process.env.GROQ_API_KEY || "YOUR_GROQ_API_KEY_HERE" });
const AVAILABLE_SKILLS = ["olympia-reasoning"];

// Sistema de prompts mais inteligente
const SYSTEM_PROMPT = `Voc√™ √© OlympIA ‚ö°, uma IA de racioc√≠nio l√≥gico baseada em Groq Llama 3.3.

INSTRU√á√ïES CR√çTICAS:
1. **Responda com PRECIS√ÉO** - N√ÉO alucine informa√ß√µes
2. **Seja L√ìGICO** - Sempre pense antes de responder
3. **Se n√£o souber, DIGA** - "N√£o tenho informa√ß√£o sobre isso" √© melhor que alucina√ß√£o
4. **DETECTE O CONTEXTO** - Se o usu√°rio pedir algo que tenha um comando, SUGIRA-O
5. **RESPONDA DE FORMA CONCISA** - Evite respostas muito longas

COMANDOS DISPON√çVEIS (quando o usu√°rio solicitar):
- /gerar - Criar conte√∫do criativo
- /analisar - An√°lise profunda
- /keywords - Extrair palavras-chave
- /imagem - Gerar imagens
- /google - Pesquisar na internet
- /traduzir - Traduzir textos
- /email - Enviar emails
- /pdf - Gerar PDFs
- /casa - Controlar casa inteligente

Se o usu√°rio pedir algo relacionado a esses comandos, SUGIRA qual usar.
Exemplo: "Isso parece uma tarefa de gera√ß√£o de conte√∫do! Use /gerar para isso."

NUNCA simule resultados de buscas, tradu√ß√µes ou imagens que n√£o pode gerar.`;

class OlympIAServer {
  constructor() {
    this.server = new Server({ name: "olympia", version: "1.0.0" }, { capabilities: { tools: {} } });
    this.setupHandlers();
  }
  
  setupHandlers() {
    this.server.setRequestHandler(ListToolsRequestSchema, async () => ({ 
      tools: [
        { name: "list_skills", description: "List skills", inputSchema: { type: "object", properties: {} } }, 
        { name: "olympia_reasoning", description: "Resposta inteligente com racioc√≠nio l√≥gico", inputSchema: { type: "object", properties: { prompt: { type: "string" } }, required: ["prompt"] } },
        { name: "nano_banana_pro_generate", description: "Generate creative text content", inputSchema: { type: "object", properties: { prompt: { type: "string" }, temperature: { type: "number" }, max_tokens: { type: "number" } }, required: ["prompt"] } },
        { name: "nano_banana_pro_analyze", description: "Analyze text for sentiment or keywords", inputSchema: { type: "object", properties: { text: { type: "string" }, analysis_type: { type: "string", enum: ["sentiment", "keywords"] } }, required: ["text", "analysis_type"] } }
      ] 
    }));
    
    this.server.setRequestHandler(CallToolRequestSchema, async (req) => {
      if (req.params.name === "list_skills") return { content: [{ type: "text", text: "‚ö° OlympIA Skills:\n- Reasoning l√≥gico e preciso\n- Detec√ß√£o de contexto\n- Sugest√µes de comandos\n- An√°lise racional\n- Gera√ß√£o de texto criativo\n- An√°lise de sentimento\n- Extra√ß√£o de palavras-chave" }] };
      
      if (req.params.name === "olympia_reasoning") {
        try {
          const completion = await groq.chat.completions.create({
            messages: [
              { role: "system", content: SYSTEM_PROMPT },
              { role: "user", content: req.params.arguments.prompt }
            ],
            model: "llama-3.3-70b-versatile",
            temperature: 0.5, // Menor para respostas mais precisas
            max_tokens: 1024,
          });
          const text = completion.choices[0]?.message?.content || "Sem resposta";
          return { content: [{ type: "text", text: `‚ö° OlympIA:\n\n${text}` }] };
        } catch (e) {
          return { content: [{ type: "text", text: `‚ùå Erro: ${e.message}` }] };
        }
      }
      
      if (req.params.name === "nano_banana_pro_generate") {
        try {
          const { prompt, temperature = 0.8, max_tokens = 500 } = req.params.arguments;
          const completion = await groq.chat.completions.create({
            messages: [
              { role: "system", content: "Voc√™ √© um assistente criativo especialista em gerar conte√∫do de alta qualidade. Gere respostas criativas, √∫teis e bem estruturadas." },
              { role: "user", content: prompt }
            ],
            model: "llama-3.3-70b-versatile",
            temperature: temperature,
            max_tokens: max_tokens,
          });
          const text = completion.choices[0]?.message?.content || "N√£o foi poss√≠vel gerar o conte√∫do.";
          return { content: [{ type: "text", text: `üçå ${text}` }] };
        } catch (e) {
          return { content: [{ type: "text", text: `‚ùå Erro na gera√ß√£o: ${e.message}` }] };
        }
      }
      
      if (req.params.name === "nano_banana_pro_analyze") {
        try {
          const { text, analysis_type } = req.params.arguments;
          
          if (analysis_type === "sentiment") {
            const completion = await groq.chat.completions.create({
              messages: [
                { role: "system", content: "Voc√™ √© um analista de sentimento especialista. Analise o texto fornecido e determine se o sentimento √© positivo, negativo ou neutro. Forne√ßa uma explica√ß√£o breve." },
                { role: "user", content: `Analise o sentimento deste texto: "${text}"` }
              ],
              model: "llama-3.3-70b-versatile",
              temperature: 0.3,
              max_tokens: 200,
            });
            const result = completion.choices[0]?.message?.content || "An√°lise n√£o dispon√≠vel.";
            return { content: [{ type: "text", text: `üìä An√°lise de Sentimento:\n${result}` }] };
          }
          
          if (analysis_type === "keywords") {
            const completion = await groq.chat.completions.create({
              messages: [
                { role: "system", content: "Voc√™ √© um especialista em extra√ß√£o de palavras-chave. Extraia as 5-10 palavras-chave mais relevantes do texto fornecido. Liste-as separadas por v√≠rgulas." },
                { role: "user", content: `Extraia palavras-chave deste texto: "${text}"` }
              ],
              model: "llama-3.3-70b-versatile",
              temperature: 0.3,
              max_tokens: 100,
            });
            const result = completion.choices[0]?.message?.content || "Palavras-chave n√£o encontradas.";
            return { content: [{ type: "text", text: `üîë Palavras-chave:\n${result}` }] };
          }
          
          return { content: [{ type: "text", text: "‚ùå Tipo de an√°lise n√£o suportado." }] };
        } catch (e) {
          return { content: [{ type: "text", text: `‚ùå Erro na an√°lise: ${e.message}` }] };
        }
      }
    });
  }
  
  async run() {
    await this.server.connect(new StdioServerTransport());
    console.error("OlympIA running");
  }
}

new OlympIAServer().run().catch(console.error);
